{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/francesco/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28, 1)\n",
      "(60000,)\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), _ = mnist.load_data()\n",
    "x_train = x_train / 255.0\n",
    "\n",
    "x_train = np.expand_dims(x_train, axis=3)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "plt.imshow(np.squeeze(x_train[10]))\n",
    "plt.show()\n",
    "print(y_train[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def generator(z, y, reuse=False, verbose=True):\\n    \\n    with tf.variable_scope(\"generator\", reuse=reuse):\\n        # Concatenate noise and conditional one-hot variable\\n        inputs = tf.concat([z, y], 1)\\n\\n        # FC layer\\n        fc1 = tf.layers.dense(inputs=inputs, units= 7 * 7 * 128, activation=tf.nn.leaky_relu)\\n\\n        reshaped = tf.reshape(fc1, shape=[-1, 7, 7, 128])\\n\\n        upconv1 = tf.layers.conv2d_transpose(inputs=reshaped,\\n                                            filters=32,\\n                                            kernel_size=[5,5],\\n                                            kernel_initializer=tf.truncated_normal_initializer(stddev=0.1),\\n                                            strides=[2,2],\\n                                            activation=tf.nn.leaky_relu,\\n                                            padding=\\'same\\', \\n                                            name=\\'upscore1\\')\\n        \\n        upconv2 = tf.layers.conv2d_transpose(inputs=upconv1,\\n                                            filters=1,\\n                                            kernel_size=[3,3],\\n                                            kernel_initializer=tf.truncated_normal_initializer(stddev=0.1),\\n                                            strides=[2,2],\\n                                            activation=None,\\n                                            padding=\\'same\\', \\n                                            name=\\'upscore2\\')\\n        \\n        prob = tf.nn.sigmoid(upconv2)\\n        \\n        if verbose:\\n            print(\"\\nGenerator:\")\\n            print(inputs)\\n            print(fc1)\\n            print(reshaped)\\n            print(upconv1)\\n            print(upconv2)\\n    \\n    return prob\\n\\n\\ndef discriminator(x, y, reuse=False, verbose=True):\\n    \\n    with tf.variable_scope(\"discriminator\", reuse=reuse):      \\n        \\n        conv1 = tf.layers.conv2d(inputs=x, \\n                                filters=64, \\n                                kernel_size=[5,5], \\n                                kernel_initializer=tf.truncated_normal_initializer(stddev=0.1),\\n                                strides=[1,1], \\n                                activation=tf.nn.leaky_relu, \\n                                padding=\\'same\\', \\n                                name=\\'Conv1\\')\\n        \\n        pool1 = tf.layers.max_pooling2d(inputs=conv1, \\n                                        pool_size=[2,2], \\n                                        strides=[2,2], \\n                                        padding=\\'same\\', \\n                                        name=\\'Pool1\\')\\n        \\n        \\n        conv2 = tf.layers.conv2d(inputs=pool1, \\n                                filters=32, \\n                                kernel_size=[3,3], \\n                                kernel_initializer=tf.truncated_normal_initializer(stddev=0.1),\\n                                strides=[2,2], \\n                                activation=tf.nn.leaky_relu,  \\n                                padding=\\'same\\', \\n                                name=\\'Conv2\\')\\n        \\n        pool2 = tf.layers.max_pooling2d(inputs=conv2, \\n                                        pool_size=[2,2], \\n                                        strides=[2,2], \\n                                        padding=\\'same\\', \\n                                        name=\\'Pool2\\')\\n\\n        flattened = tf.layers.flatten(pool2)\\n        concatened = tf.concat([flattened, y], 1)\\n        fc1 = tf.layers.dense(inputs=concatened, units=256, activation=tf.nn.leaky_relu)\\n        fc2 = tf.layers.dense(inputs=fc1, units=1, activation=None)\\n        prob = tf.nn.sigmoid(fc2)\\n        \\n        if verbose:\\n            print(\"\\nDiscriminator:\")\\n            print(conv1)\\n            print(pool1)\\n            print(conv2)\\n            print(pool2)\\n            print(flattened)\\n            print(concatened)\\n            print(fc1)\\n            print(fc2)\\n        \\n    return prob, fc2'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''def generator(z, y, reuse=False, verbose=True):\n",
    "    \n",
    "    with tf.variable_scope(\"generator\", reuse=reuse):\n",
    "        # Concatenate noise and conditional one-hot variable\n",
    "        inputs = tf.concat([z, y], 1)\n",
    "\n",
    "        # FC layer\n",
    "        fc1 = tf.layers.dense(inputs=inputs, units= 7 * 7 * 128, activation=tf.nn.leaky_relu)\n",
    "\n",
    "        reshaped = tf.reshape(fc1, shape=[-1, 7, 7, 128])\n",
    "\n",
    "        upconv1 = tf.layers.conv2d_transpose(inputs=reshaped,\n",
    "                                            filters=32,\n",
    "                                            kernel_size=[5,5],\n",
    "                                            kernel_initializer=tf.truncated_normal_initializer(stddev=0.1),\n",
    "                                            strides=[2,2],\n",
    "                                            activation=tf.nn.leaky_relu,\n",
    "                                            padding='same', \n",
    "                                            name='upscore1')\n",
    "        \n",
    "        upconv2 = tf.layers.conv2d_transpose(inputs=upconv1,\n",
    "                                            filters=1,\n",
    "                                            kernel_size=[3,3],\n",
    "                                            kernel_initializer=tf.truncated_normal_initializer(stddev=0.1),\n",
    "                                            strides=[2,2],\n",
    "                                            activation=None,\n",
    "                                            padding='same', \n",
    "                                            name='upscore2')\n",
    "        \n",
    "        prob = tf.nn.sigmoid(upconv2)\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"\\nGenerator:\")\n",
    "            print(inputs)\n",
    "            print(fc1)\n",
    "            print(reshaped)\n",
    "            print(upconv1)\n",
    "            print(upconv2)\n",
    "    \n",
    "    return prob\n",
    "\n",
    "\n",
    "def discriminator(x, y, reuse=False, verbose=True):\n",
    "    \n",
    "    with tf.variable_scope(\"discriminator\", reuse=reuse):      \n",
    "        \n",
    "        conv1 = tf.layers.conv2d(inputs=x, \n",
    "                                filters=64, \n",
    "                                kernel_size=[5,5], \n",
    "                                kernel_initializer=tf.truncated_normal_initializer(stddev=0.1),\n",
    "                                strides=[1,1], \n",
    "                                activation=tf.nn.leaky_relu, \n",
    "                                padding='same', \n",
    "                                name='Conv1')\n",
    "        \n",
    "        pool1 = tf.layers.max_pooling2d(inputs=conv1, \n",
    "                                        pool_size=[2,2], \n",
    "                                        strides=[2,2], \n",
    "                                        padding='same', \n",
    "                                        name='Pool1')\n",
    "        \n",
    "        \n",
    "        conv2 = tf.layers.conv2d(inputs=pool1, \n",
    "                                filters=32, \n",
    "                                kernel_size=[3,3], \n",
    "                                kernel_initializer=tf.truncated_normal_initializer(stddev=0.1),\n",
    "                                strides=[2,2], \n",
    "                                activation=tf.nn.leaky_relu,  \n",
    "                                padding='same', \n",
    "                                name='Conv2')\n",
    "        \n",
    "        pool2 = tf.layers.max_pooling2d(inputs=conv2, \n",
    "                                        pool_size=[2,2], \n",
    "                                        strides=[2,2], \n",
    "                                        padding='same', \n",
    "                                        name='Pool2')\n",
    "\n",
    "        flattened = tf.layers.flatten(pool2)\n",
    "        concatened = tf.concat([flattened, y], 1)\n",
    "        fc1 = tf.layers.dense(inputs=concatened, units=256, activation=tf.nn.leaky_relu)\n",
    "        fc2 = tf.layers.dense(inputs=fc1, units=1, activation=None)\n",
    "        prob = tf.nn.sigmoid(fc2)\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"\\nDiscriminator:\")\n",
    "            print(conv1)\n",
    "            print(pool1)\n",
    "            print(conv2)\n",
    "            print(pool2)\n",
    "            print(flattened)\n",
    "            print(concatened)\n",
    "            print(fc1)\n",
    "            print(fc2)\n",
    "        \n",
    "    return prob, fc2'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_Z(batch_size, img_size):\n",
    "    # Sample noise for generator\n",
    "    return np.random.uniform(-1., 1., size=[batch_size, img_size])\n",
    "\n",
    "\n",
    "def one_hot(batch_size, num_classes, labels):\n",
    "    assert(batch_size == len(labels))\n",
    "    y_one_hot = np.zeros(shape=[batch_size, num_classes])\n",
    "    y_one_hot[np.arange(batch_size), labels] = 1\n",
    "    \n",
    "    return y_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(z, y, reuse=False, verbose=True):\n",
    "    \n",
    "    with tf.variable_scope(\"generator\", reuse=reuse):\n",
    "        inputs = tf.concat([z, y], 1)\n",
    "        \n",
    "        fc1 = tf.layers.dense(inputs=inputs, units=256, activation=tf.nn.leaky_relu)\n",
    "        fc2 = tf.layers.dense(inputs=fc1, units=784, activation=None)\n",
    "        logits = tf.nn.sigmoid(fc2)\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"\\nGenerator:\")\n",
    "            print(inputs)\n",
    "            print(fc1)\n",
    "            print(fc2)\n",
    "        \n",
    "    return logits\n",
    "\n",
    "\n",
    "def discriminator(x, y, reuse=False, verbose=True):\n",
    "    \n",
    "    with tf.variable_scope(\"discriminator\", reuse=reuse):\n",
    "        inputs = tf.concat([x, y], 1)\n",
    "        fc1 = tf.layers.dense(inputs=inputs, units=256, activation=tf.nn.leaky_relu)\n",
    "        fc2 = tf.layers.dense(inputs=fc1, units=1, activation=None)\n",
    "        prob = tf.nn.sigmoid(fc2)\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"\\nDiscriminator:\")\n",
    "            print(inputs)\n",
    "            print(fc1)\n",
    "            print(fc2)\n",
    "        \n",
    "    return prob, fc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:\n",
      "Discriminator input: Tensor(\"X:0\", shape=(?, 784), dtype=float32)\n",
      "Conditional variable: Tensor(\"Y:0\", shape=(?, 10), dtype=float32)\n",
      "Generator input noise: Tensor(\"Z:0\", shape=(?, 100), dtype=float32)\n",
      "\n",
      "Generator:\n",
      "Tensor(\"generator/concat:0\", shape=(?, 110), dtype=float32)\n",
      "Tensor(\"generator/dense/LeakyRelu:0\", shape=(?, 256), dtype=float32)\n",
      "Tensor(\"generator/dense_1/BiasAdd:0\", shape=(?, 784), dtype=float32)\n",
      "\n",
      "Discriminator:\n",
      "Tensor(\"discriminator/concat:0\", shape=(?, 794), dtype=float32)\n",
      "Tensor(\"discriminator/dense/LeakyRelu:0\", shape=(?, 256), dtype=float32)\n",
      "Tensor(\"discriminator/dense_1/BiasAdd:0\", shape=(?, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Discriminator input\n",
    "#X = tf.placeholder(tf.float32, shape=[None, x_train.shape[1], x_train.shape[2], 1], name='X')\n",
    "X = tf.placeholder(tf.float32, shape=[None, x_train.shape[1] * x_train.shape[2] * 1], name='X')\n",
    "\n",
    "# Generator noise input\n",
    "Z_dim = 100\n",
    "Z = tf.placeholder(tf.float32, shape=[None, Z_dim], name='Z')\n",
    "\n",
    "# Generator conditional\n",
    "Y = tf.placeholder(tf.float32, shape=[None, num_classes], name='Y')\n",
    "\n",
    "# Print shapes\n",
    "print(\"Inputs:\")\n",
    "print(\"Discriminator input: \" + str(X))\n",
    "print(\"Conditional variable: \" + str(Y))\n",
    "print(\"Generator input noise: \" + str(Z))\n",
    "\n",
    "# Networks\n",
    "gen_sample = generator(Z, Y)\n",
    "D_real, D_logit_real = discriminator(X, Y)\n",
    "D_fake, D_logit_fake = discriminator(gen_sample, Y, reuse=True, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theoretical remark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary cross entropy loss\n",
    "\\begin{equation*}\n",
    "L(\\theta) = - \\frac{1}{n} \\sum_{i=1}^n [y_i log(p_i) + (1 - y_i) log(1 - p_i)]\n",
    "\\end{equation*}\n",
    "\n",
    "- Discriminator final probability is 1 => REAL IMAGE\n",
    "- Discriminator final probability is 0 => FAKE IMAGE\n",
    "\n",
    "Log values:\n",
    "- Log(1) => Loss would be 0\n",
    "- Log(0+) => Loss would be to - ∞\n",
    "\n",
    "### Generator:\n",
    "\n",
    "Maximize D(G(z))\n",
    "\n",
    "\n",
    "### Discriminator:\n",
    "\n",
    "Maximize D(x) AND minimize D(G(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Losses have minus sign because I have to maximize them\n",
    "D_loss = - tf.reduce_mean( tf.log(D_real) + tf.log(1. - D_fake) )\n",
    "G_loss = - tf.reduce_mean( tf.log(D_fake) )\n",
    "\n",
    "# Optimizers\n",
    "D_var = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='discriminator')\n",
    "D_optimizer = tf.train.AdamOptimizer(learning_rate=0.0005).minimize(D_loss, var_list=D_var)\n",
    "\n",
    "G_var = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='generator')\n",
    "G_optimizer = tf.train.AdamOptimizer(learning_rate=0.0005).minimize(G_loss, var_list=G_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training of the generator and discriminator network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "0.95421404  2.7137277\n",
      "0.16300228  4.1718526\n",
      "0.013767928  5.6764627\n",
      "0.013310271  6.8244886\n",
      "Epoch 2\n",
      "0.006558732  8.058235\n",
      "0.0042580855  10.943196\n",
      "0.0054634595  8.849958\n",
      "0.04933323  3.9225428\n",
      "Epoch 3\n",
      "0.20747188  4.1831617\n",
      "0.026214357  8.469037\n",
      "0.04072192  7.346262\n",
      "0.031122517  5.530775\n",
      "Epoch 4\n",
      "0.046090726  5.11003\n",
      "0.10653249  6.1811914\n",
      "0.17848419  4.4241347\n",
      "0.08084142  4.8053803\n",
      "Epoch 5\n",
      "0.23476085  5.18079\n",
      "0.07258764  5.5152216\n",
      "0.059076965  5.4870343\n",
      "0.06739945  5.7131615\n",
      "Epoch 6\n",
      "0.014707347  8.53326\n",
      "0.061251864  6.100748\n",
      "0.046907976  5.397523\n",
      "0.050764  6.3975506\n",
      "Epoch 7\n",
      "0.12136417  6.4027653\n",
      "0.11473328  5.735597\n",
      "0.12664294  5.3891563\n",
      "0.21780637  5.6415553\n",
      "Epoch 8\n",
      "0.16854784  6.121756\n",
      "0.16422081  4.4690456\n",
      "0.12301879  5.350264\n",
      "0.16067845  4.7910204\n",
      "Epoch 9\n",
      "0.22020595  5.8412743\n",
      "0.20873132  3.664666\n",
      "0.23085178  4.1530704\n",
      "0.2533369  4.857728\n",
      "Epoch 10\n",
      "0.24765852  4.557816\n",
      "0.22004695  3.4729404\n",
      "0.2738728  4.0640154\n",
      "0.40452164  4.2124043\n",
      "Epoch 11\n",
      "0.3247204  3.8793058\n",
      "0.41095752  3.597\n",
      "0.470631  2.990757\n",
      "0.470249  2.769702\n",
      "Epoch 12\n",
      "0.4585572  3.290328\n",
      "0.44131833  3.5321622\n",
      "0.449372  2.8683414\n",
      "0.41017365  2.880132\n",
      "Epoch 13\n",
      "0.46287364  3.1273148\n",
      "0.43746972  2.8026934\n",
      "0.4182534  2.9048421\n",
      "0.51053816  3.5538886\n",
      "Epoch 14\n",
      "0.51813316  3.352604\n",
      "0.41244042  2.688943\n",
      "0.4439724  2.6302154\n",
      "0.5396354  3.5026073\n",
      "Epoch 15\n",
      "0.5260041  2.417328\n",
      "0.48333907  3.2186046\n",
      "0.41663694  3.8180304\n",
      "0.5617603  3.5638638\n",
      "Epoch 16\n",
      "0.44995552  2.9590058\n",
      "0.51086545  2.5292003\n",
      "0.41565672  3.0935984\n",
      "0.54793745  3.145947\n",
      "Epoch 17\n",
      "0.72692466  2.1899939\n",
      "0.80327255  2.0908456\n",
      "0.60116035  3.2365272\n",
      "0.51467735  2.7812507\n",
      "Epoch 18\n",
      "0.42777276  2.6575816\n",
      "0.43597516  2.9297204\n",
      "0.63173753  2.6818922\n",
      "0.45614332  3.1636071\n",
      "Epoch 19\n",
      "0.6495122  3.1494155\n",
      "0.4465957  2.3796701\n",
      "0.50720423  2.1689343\n",
      "0.4916851  2.8152337\n",
      "Epoch 20\n",
      "0.51391214  2.6834903\n",
      "0.534864  2.8063793\n",
      "0.54428625  2.5500538\n",
      "0.50708604  2.8288333\n",
      "Epoch 21\n",
      "0.649691  2.5629652\n",
      "0.49599117  3.0103395\n",
      "0.50717974  2.4034843\n",
      "0.51282674  3.0892022\n",
      "Epoch 22\n",
      "0.64791065  2.2763448\n",
      "0.41610396  3.06706\n",
      "0.5095854  2.1509743\n",
      "0.43552715  2.736646\n",
      "Epoch 23\n",
      "0.5028825  2.349042\n",
      "0.5256474  2.818008\n",
      "0.71356356  2.1300406\n",
      "0.54162276  2.7645075\n",
      "Epoch 24\n",
      "0.6501664  2.8713064\n",
      "0.38340417  2.6284382\n",
      "0.62390625  2.049768\n",
      "0.5201373  2.7279048\n",
      "Epoch 25\n",
      "0.6313869  2.3506942\n",
      "0.5935847  2.646382\n",
      "0.46828043  2.7511795\n",
      "0.4332309  2.8218865\n",
      "Epoch 26\n",
      "0.72029173  1.9618573\n",
      "0.47048056  2.8291447\n",
      "0.50303066  2.4152875\n",
      "0.46554768  2.657115\n",
      "Epoch 27\n",
      "0.7475612  1.990246\n",
      "0.6741786  2.9049273\n",
      "0.59012496  2.116159\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    # Run the initializer\n",
    "    saver = tf.train.Saver()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Epochs\n",
    "    for n in range(30):\n",
    "        print(\"Epoch \" + str(n+1))\n",
    "        \n",
    "        for i in range(len(x_train) // batch_size):\n",
    "            \n",
    "            X_tmp = np.reshape(x_train[i*batch_size:(i+1)*batch_size], (batch_size, -1))\n",
    "            #X_tmp = x_train[i*batch_size:(i+1)*batch_size]\n",
    "            \n",
    "            sampled_noise = sample_Z(batch_size, Z_dim)\n",
    "            one_hot_sampled = one_hot(batch_size, num_classes, y_train[i*batch_size:(i+1)*batch_size])\n",
    "            \n",
    "            _, D_loss_val = sess.run([D_optimizer, D_loss], feed_dict={X: X_tmp, \n",
    "                                                                       Y: one_hot_sampled,\n",
    "                                                                       Z: sampled_noise})\n",
    "\n",
    "            _, G_loss_val = sess.run([G_optimizer, G_loss], feed_dict={Y: one_hot_sampled,\n",
    "                                                                       Z: sampled_noise})\n",
    "            \n",
    "            if i % 300 == 0:\n",
    "                print(str(D_loss_val) + \"  \" + str(G_loss_val))\n",
    "                save_path = saver.save(sess, \"./checkpoints/model.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate images with constraint (Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_number = 8\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver.restore(sess, \"./checkpoints/model.ckpt\")\n",
    "    \n",
    "    sampled_noise = sample_Z(1, Z_dim)\n",
    "    one_hot_sampled = one_hot(1, num_classes, [generate_number])\n",
    "    \n",
    "    generated = sess.run(gen_sample, feed_dict={Y: one_hot_sampled, \n",
    "                                                Z: sampled_noise})\n",
    "    img_generated = np.reshape(generated, (28, 28))\n",
    "    \n",
    "    plt.imshow(img_generated)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
